{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ca92e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of tokens in dataset: 2.56M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3208c8cd2254498bc681ac3bcde2bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b266c7dea9ac4fdcaead07447ca8c2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e9cc5e37734d5eb6a14f957415c6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf208acdc254266abc52916797beb64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "# Example usage:\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model_name = \"gpt2\"\n",
    "# model_name = \"HuggingFaceTB/SmolLM-360M\"\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "m_name_save= model_name.replace(\"/\", \"_\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_bos_token = True\n",
    "# batch_size = 512\n",
    "# batch_size = 32\n",
    "batch_size =32\n",
    "max_length = 128\n",
    "learning_rate = 1e-3\n",
    "if(model_name == \"gpt2\"):\n",
    "    target_layer = 'transformer.h.5'\n",
    "    d_name = None\n",
    "else: \n",
    "    target_layer = \"model.layers.18\"\n",
    "    d_name = \"cosmopedia-v2\"\n",
    "debug = True\n",
    "if(debug):\n",
    "    if(model_name == \"gpt2\"):\n",
    "        dataset_name = \"Elriggs/openwebtext-100k\"\n",
    "    else: \n",
    "        dataset_name = \"HuggingFaceTB/smollm-corpus\"\n",
    "    # num_datapoints = 1_000_000\n",
    "    num_datapoints = 20_000\n",
    "    # num_datapoints = 15_000\n",
    "    # num_datapoints = 2_000\n",
    "    total_batches = num_datapoints // batch_size\n",
    "    print(f\"total amount of tokens in dataset: {num_datapoints * max_length / 1e6}M\")\n",
    "else:    \n",
    "    if(model_name == \"gpt2\"):\n",
    "        dataset_name = \"prithivMLmods/OpenWeb888K\"\n",
    "        num_datapoints = 888_000 # 880_000\n",
    "        total_batches = 888_000 // batch_size\n",
    "    else: \n",
    "        dataset_name = \"HuggingFaceTB/smollm-corpus\"\n",
    "        num_datapoints = 2_000_000\n",
    "        total_batches = num_datapoints // batch_size\n",
    "        print(f\"total amount of tokens in dataset: {num_datapoints * max_length / 1e6}M\")\n",
    "\n",
    "data_generator = TokenizedDataset(dataset_name, tokenizer, d_name, batch_size=batch_size, max_length=max_length, total_batches=total_batches)\n",
    "\n",
    "val_data_generator = TokenizedDataset(dataset_name, tokenizer, d_name, batch_size=batch_size, max_length=max_length, total_batches=total_batches, shuffle_seed=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f18f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 0.51 GB\n",
      "Memory usage: 0.65 GB\n"
     ]
    }
   ],
   "source": [
    "# Now we want to download all our SAEs\n",
    "import json\n",
    "from utils import DotDict, AutoEncoderTopK\n",
    "from huggingface_hub import hf_hub_download\n",
    "huggingface_name = \"Elriggs/seq_concat_HuggingFaceTB_SmolLM-135M_model.layers.18\"\n",
    "name_prefix = f\"sae_k=30_tokBias=True\"\n",
    "sae_name_style = name_prefix + \".pt\"\n",
    "cfg_name_style = name_prefix + \"_cfg.json\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=huggingface_name,\n",
    "    filename=sae_name_style\n",
    ")\n",
    "\n",
    "# Download config file\n",
    "config_path = hf_hub_download(\n",
    "    repo_id=huggingface_name,\n",
    "    filename=cfg_name_style\n",
    ")\n",
    "cfg = DotDict(json.load(open(config_path)))\n",
    "\n",
    "sae = AutoEncoderTopK.from_pretrained(model_path, k=cfg.k, device = None, embedding=True)\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "sae.to(device)\n",
    "# print memory  in GB\n",
    "print(f\"Memory usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c151f8",
   "metadata": {},
   "source": [
    "# Train SAE offset by 1 batch \n",
    "so we don't need to recompute the LLM's activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcaaad7",
   "metadata": {},
   "source": [
    "# Logan's Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c5544cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemperatureScheduler:\n",
    "    def __init__(self, \n",
    "                 high_temp=1e-3,\n",
    "                 low_temp=0.0,\n",
    "                 start_high_fraction=0.2,  # First 20% at high temp\n",
    "                 end_high_fraction=0.6,    # High temp until 60% of training\n",
    "                 end_transition_fraction=0.8):  # Low temp from 80% to end\n",
    "        \"\"\"\n",
    "        Temperature scheduler that controls noise injection during training.\n",
    "        \n",
    "        Args:\n",
    "            high_temp: The higher temperature value\n",
    "            low_temp: The lower temperature value (typically 0)\n",
    "            start_high_fraction: Fraction of training where temp reaches high_temp\n",
    "            end_high_fraction: Fraction of training where temp starts to decrease\n",
    "            end_transition_fraction: Fraction of training where temp reaches low_temp\n",
    "        \"\"\"\n",
    "        self.high_temp = high_temp\n",
    "        self.low_temp = low_temp\n",
    "        self.start_high_fraction = start_high_fraction\n",
    "        self.end_high_fraction = end_high_fraction\n",
    "        self.end_transition_fraction = end_transition_fraction\n",
    "        self.current_state = None\n",
    "        \n",
    "    def get_temperature(self, current_step, total_steps):\n",
    "        \"\"\"\n",
    "        Calculate the temperature for the current training step.\n",
    "        \n",
    "        Args:\n",
    "            current_step: Current training step (0-indexed)\n",
    "            total_steps: Total number of training steps\n",
    "            \n",
    "        Returns:\n",
    "            The temperature value for the current step\n",
    "        \"\"\"\n",
    "        # Convert to fraction of total training\n",
    "        progress = current_step / total_steps\n",
    "        \n",
    "        # Phase 1: Ramp up from low_temp to high_temp\n",
    "        if progress < self.start_high_fraction:\n",
    "            self.current_state = \"ramping_up\"\n",
    "            # Linear interpolation from low_temp to high_temp\n",
    "            alpha = progress / self.start_high_fraction\n",
    "            return self.low_temp + (self.high_temp - self.low_temp) * alpha\n",
    "        \n",
    "        # Phase 2: Stay at high_temp\n",
    "        elif progress < self.end_high_fraction:\n",
    "            self.current_state = \"high_temp\"\n",
    "            return self.high_temp\n",
    "        \n",
    "        # Phase 3: Ramp down from high_temp to low_temp\n",
    "        elif progress < self.end_transition_fraction:\n",
    "            self.current_state = \"ramping_down\"\n",
    "            # Linear interpolation from high_temp back to low_temp\n",
    "            alpha = (progress - self.end_high_fraction) / (self.end_transition_fraction - self.end_high_fraction)\n",
    "            return self.high_temp - (self.high_temp - self.low_temp) * alpha\n",
    "        \n",
    "        # Phase 4: Stay at low_temp until the end\n",
    "        else:\n",
    "            self.current_state = \"low_temp\"\n",
    "            return self.low_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c99bb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/625 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/625, CE diff: 0.000, fvu: 0.117 | temperature: 0.0e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 26/625 [00:33<09:04,  1.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 25/625, CE diff: -0.001, fvu: 0.113 | temperature: 1.6e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 50/625 [00:55<08:53,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/625, CE diff: -0.000, fvu: 0.114 | temperature: 3.2e-04\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      " 12%|█▏        | 76/625 [01:42<08:31,  1.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 75/625, CE diff: 0.011, fvu: 0.129 | temperature: 4.0e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 100/625 [02:05<08:15,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/625, CE diff: 0.018, fvu: 0.118 | temperature: 4.0e-04\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n",
      " 20%|██        | 126/625 [02:52<07:50,  1.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 125/625, CE diff: 0.029, fvu: 0.123 | temperature: 4.0e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 150/625 [03:15<07:30,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/625, CE diff: 0.034, fvu: 0.116 | temperature: 4.0e-04\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.18it/s]\n",
      " 28%|██▊       | 176/625 [04:02<07:05,  1.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 175/625, CE diff: 0.043, fvu: 0.121 | temperature: 4.0e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 200/625 [04:25<06:46,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/625, CE diff: 0.058, fvu: 0.121 | temperature: 4.0e-04\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      " 36%|███▌      | 226/625 [05:11<06:15,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 225/625, CE diff: 0.038, fvu: 0.117 | temperature: 4.0e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 250/625 [05:34<05:59,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/625, CE diff: 0.050, fvu: 0.125 | temperature: 4.0e-04\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      " 44%|████▍     | 276/625 [06:21<05:30,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 275/625, CE diff: 0.073, fvu: 0.124 | temperature: 4.0e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 300/625 [06:43<05:11,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/625, CE diff: 0.071, fvu: 0.124 | temperature: 4.0e-04\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      " 52%|█████▏    | 326/625 [07:30<04:42,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 325/625, CE diff: 0.062, fvu: 0.125 | temperature: 3.2e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 350/625 [07:53<04:22,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 350/625, CE diff: 0.066, fvu: 0.127 | temperature: 1.6e-04\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      " 60%|██████    | 376/625 [08:40<03:54,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 375/625, CE diff: 0.064, fvu: 0.126 | temperature: 0.0e+00\n",
      "Restarting Adam optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 400/625 [09:03<03:37,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400/625, CE diff: 0.067, fvu: 0.134 | temperature: 0.0e+00\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      " 68%|██████▊   | 426/625 [09:50<03:10,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 425/625, CE diff: 0.043, fvu: 0.129 | temperature: 0.0e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 450/625 [10:13<02:48,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 450/625, CE diff: 0.016, fvu: 0.130 | temperature: 0.0e+00\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.18it/s]\n",
      " 76%|███████▌  | 476/625 [11:00<02:22,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 475/625, CE diff: 0.029, fvu: 0.120 | temperature: 0.0e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 500/625 [11:23<02:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/625, CE diff: 0.024, fvu: 0.128 | temperature: 0.0e+00\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      " 84%|████████▍ | 526/625 [12:10<01:34,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 525/625, CE diff: 0.008, fvu: 0.126 | temperature: 0.0e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 550/625 [12:34<01:12,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 550/625, CE diff: 0.002, fvu: 0.119 | temperature: 0.0e+00\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\n",
      " 92%|█████████▏| 576/625 [13:22<00:46,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 575/625, CE diff: 0.008, fvu: 0.118 | temperature: 0.0e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 600/625 [13:45<00:24,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600/625, CE diff: -0.002, fvu: 0.131 | temperature: 0.0e+00\n",
      "Saving feature activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.18it/s]\n",
      "100%|██████████| 625/625 [14:31<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def calc_ce_loss(logits, batch, reduction=\"none\"):\n",
    "    # first shift each by one. No ground truth for last token, no prediction for first token\n",
    "    shifted_logits = logits[:, :-1]  # Remove prediction for last token\n",
    "    shifted_targets = batch[:, 1:]   # Remove ground truth for first token\n",
    "    \n",
    "    batch_size, seq_len, vocab_size = shifted_logits.shape\n",
    "    #print batch size\n",
    "    \n",
    "\n",
    "    ce_loss = F.cross_entropy(\n",
    "        shifted_logits.contiguous().view(-1, shifted_logits.size(-1)),\n",
    "        shifted_targets.contiguous().view(-1),\n",
    "        reduction=reduction\n",
    "    )\n",
    "    ce_loss = ce_loss.view(batch_size, seq_len)\n",
    "    return ce_loss\n",
    "\n",
    "def update_feature_sim(feature_sim, per_feature_MSE, local_ids, num_features, batch_idx):\n",
    "    # find out how many times each feature index shows up in local_ids\n",
    "    num_features = post_relu_feat_acts_BF.shape[-1]\n",
    "    # Get unique indices and their counts\n",
    "    unique_indices, counts = torch.unique(local_ids, return_counts=True)\n",
    "\n",
    "    unique_counts = torch.zeros(num_features, device=local_ids.device)\n",
    "    unique_counts.index_put_((unique_indices,), counts.float())\n",
    "\n",
    "    feature_mse_sum = torch.zeros(num_features, device=local_ids.device)\n",
    "    feature_mse_sum.scatter_add_(0, local_ids.flatten(), per_feature_MSE.flatten())\n",
    "\n",
    "    # Compute the average MSE for each feature by dividing by its count\n",
    "    # (avoiding division by zero for features that don't appear)\n",
    "    feature_mask = unique_counts > 0\n",
    "    feature_mse_avg = torch.zeros_like(feature_mse_sum)\n",
    "    feature_mse_avg[feature_mask] = feature_mse_sum[feature_mask] / unique_counts[feature_mask]\n",
    "\n",
    "    feature_sim[batch_idx, unique_indices] = feature_mse_avg[unique_indices].to(\"cpu\")\n",
    "    return feature_sim\n",
    "\n",
    "def sub_bias_and_normalize(x, sae, batch, normalize=True):\n",
    "    if normalize:\n",
    "        x = (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + 1e-8)\n",
    "    if sae.per_token_bias:\n",
    "        x = x - sae.per_token_bias(batch)\n",
    "    return x\n",
    "\n",
    "import math\n",
    "# lots of code used from https://github.com/bfpill/devinterp-1/blob/main/modular_addition/calc_lambda.ipynb\n",
    "# feature_sim = torch.ones(total_batches, 9216)*-0.1 # num of features\n",
    "num_features = sae.encoder.weight.shape[0]\n",
    "dataset_size = num_datapoints* max_length\n",
    "# beta_star_scalar = dataset_size / math.log(dataset_size) # 1/Temp (From App A)\n",
    "beta_star_scalar = 1\n",
    "lr = 1e-4  # \"epsilon\"\n",
    "gamma = 0  # Elasticity Regularization (diff from original weights w*)\n",
    "\n",
    "normalize = True\n",
    "\n",
    "\n",
    "#temp 5.0e-4 went t. 2.0 & never recovered (also gamm=10)\n",
    "# Temperature parameters\n",
    "# max_temp = 2.0e-4\n",
    "max_temp = 4.0e-4\n",
    "lowest_temp = 0.0\n",
    "peak_noise_fraction = 0.1\n",
    "start_decreasing_fraction = 0.5\n",
    "stay_at_zero_fraction = 0.6\n",
    "tempScheduler = TemperatureScheduler(\n",
    "    max_temp,\n",
    "    lowest_temp,\n",
    "    peak_noise_fraction,\n",
    "    start_decreasing_fraction,\n",
    "    stay_at_zero_fraction\n",
    ")\n",
    "# save_checkpoints_every = 10\n",
    "# batches_to_run = 10 #for validation\n",
    "save_checkpoints_every = 50\n",
    "batches_to_run = 20 #for validation\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "orig_model = deepcopy(model)\n",
    "orig_sae = deepcopy(sae)\n",
    "all_ce_losses = []\n",
    "# all_elasticity_losses = []\n",
    "all_ce_diffs = []\n",
    "fvus = []\n",
    "sgd_opt = torch.optim.SGD(model.parameters(), lr=1.0)\n",
    "adam_opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "sae_opt = torch.optim.Adam(sae.parameters(), lr=learning_rate)\n",
    "restarted_adam=False\n",
    "for batch_idx in tqdm(range(total_batches)):\n",
    "    current_opt = adam_opt if tempScheduler.current_state == \"low_temp\" else sgd_opt\n",
    "    current_opt.zero_grad() \n",
    "    sae_opt.zero_grad()\n",
    "    if(cfg.norm_decoder):\n",
    "        sae.set_decoder_norm_to_unit_norm()\n",
    "    batch = data_generator.next().to(device)\n",
    "    with Trace(model, target_layer) as original_trace:\n",
    "        ce_loss = model(batch, labels=batch).loss\n",
    "        new_x = original_trace.output[0] if isinstance(original_trace.output, tuple) else original_trace.output\n",
    "        new_x = new_x.detach()\n",
    "\n",
    "    # SAE\n",
    "    # w = torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    # elasticity_loss = gamma* torch.mean(((w_0 - w) ** 2))\n",
    "    elasticity_loss = 0.0\n",
    "\n",
    "    full_loss = lr/2 * (beta_star_scalar*ce_loss + elasticity_loss)\n",
    "    full_loss.backward()\n",
    "\n",
    "    if(tempScheduler.current_state == \"low_temp\" and not restarted_adam):\n",
    "        # Restart Adam optimizer\n",
    "        print(\"Restarting Adam optimizer\")\n",
    "        for state in adam_opt.state.values():\n",
    "            state.clear()\n",
    "        adam_opt = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "        restarted_adam = True\n",
    "    # Choose optimizer based on temperature state\n",
    "    current_opt.step()\n",
    "    # Store losses\n",
    "    all_ce_losses.append(ce_loss.item())\n",
    "    # all_elasticity_losses.append(elasticity_loss.item())\n",
    "\n",
    "    # SAE\n",
    "    new_x = (new_x - new_x.mean(dim=-1, keepdim=True)) / (new_x.std(dim=-1, keepdim=True) + 1e-8)\n",
    "    bias = sae.per_token_bias(batch)\n",
    "    x_hat = sae(new_x - bias) + bias\n",
    "    with torch.no_grad():\n",
    "        fvu = calculate_fvu(new_x, x_hat)\n",
    "    mse = torch.mean((x_hat - new_x) ** 2)\n",
    "    mse.backward()\n",
    "    if(cfg.norm_decoder):\n",
    "        sae.remove_gradient_parallel_to_decoder_directions()\n",
    "\n",
    "    sae_opt.step()\n",
    "\n",
    "    # Add noise to the parameters\n",
    "    with torch.no_grad():\n",
    "        if tempScheduler.current_state != \"low_temp\":\n",
    "            temperature = tempScheduler.get_temperature(batch_idx, total_batches)\n",
    "            new_params = torch.nn.utils.parameters_to_vector(model.parameters()) \n",
    "            noise = torch.randn_like(new_params) * temperature\n",
    "            torch.nn.utils.vector_to_parameters(new_params + noise, model.parameters())\n",
    "        # if batch_idx % 1 ==0:\n",
    "            # Check CE diff w/ original model \n",
    "    \n",
    "\n",
    "        \n",
    "    # SAE sims\n",
    "    with torch.no_grad():\n",
    "        # Get original model's acts & SAE features\n",
    "        with Trace(orig_model, target_layer) as original_trace:\n",
    "            original_ce_loss = orig_model(batch, labels=batch).loss\n",
    "        #     old_x = original_trace.output[0] if isinstance(original_trace.output, tuple) else original_trace.output\n",
    "        #     old_x = old_x.detach()\n",
    "        # # x = x.normalize - bias (so ready for SAE.encode)\n",
    "        # old_x = sub_bias_and_normalize(old_x, orig_sae, batch, normalize=normalize)\n",
    "\n",
    "        # _, local_mags, local_ids = orig_sae.encode(old_x, return_topk=True)\n",
    "\n",
    "             \n",
    "        # # New x is already normalized & -bias\n",
    "        # new_x = sub_bias_and_normalize(new_x, sae, batch, normalize=normalize)\n",
    "        \n",
    "        # post_relu_feat_acts_BF = nn.functional.relu(sae.encoder(new_x - sae.b_dec))\n",
    "        # # get the activations at the feature_ids (feature_ids is = to top_indices_BK)\n",
    "        # custom_feature_acts = post_relu_feat_acts_BF.gather(dim=-1, index=local_ids)\n",
    "        # per_feature_MSE = ((custom_feature_acts - local_mags)**2)\n",
    "        # feature_sim = update_feature_sim(feature_sim, per_feature_MSE, local_ids, num_features = post_relu_feat_acts_BF.shape[-1], batch_idx=batch_idx)\n",
    "        #TODO above should be squared\n",
    "\n",
    "\n",
    "        # Get CE diff\n",
    "        ce_diff = ce_loss - original_ce_loss\n",
    "        all_ce_diffs.append(ce_diff.item())\n",
    "    if batch_idx % 25 == 0:\n",
    "        # calculate fvu of the x & x_hat\n",
    "        # fvu = calculate_fvu(new_x, x_hat)\n",
    "        # fvus.append(fvu)\n",
    "    # if batch_idx % 1 == 0:\n",
    "        # do temp in einsteins notation\n",
    "        # print(f\"Batch {batch_idx}/{total_batches}, Loss: {ce_loss.item():.3f} + {elasticity_loss.item():.5f} | temperature: {temperature:.1e}\")\n",
    "        print(f\"Batch {batch_idx}/{total_batches}, CE diff: {ce_diff.item():.3f}, fvu: {fvu:.3f} | temperature: {temperature:.1e}\")\n",
    "    with torch.no_grad():\n",
    "        # save first batch info:\n",
    "        d_model = model.config.hidden_size\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        # First batch\n",
    "        if batch_idx == 0:\n",
    "            all_tokens = torch.zeros(batches_to_run*batch_size, max_length-1, dtype=torch.long)\n",
    "            val_original_ce_loss = torch.zeros(batches_to_run*batch_size, max_length-1, dtype=torch.float16)\n",
    "            all_activations = torch.zeros(batches_to_run*batch_size, max_length-1, d_model, dtype=torch.float16)\n",
    "            val_data_generator.reset_iterator()\n",
    "            for i in range(batches_to_run):\n",
    "                batch = val_data_generator.next().to(device)\n",
    "                with Trace(model, target_layer) as original_trace:\n",
    "                    original_logits = model(batch, labels=batch, reduction=\"none\").logits\n",
    "                    new_x = original_trace.output[0] if isinstance(original_trace.output, tuple) else original_trace.output\n",
    "                    new_x = new_x.detach()\n",
    "                ce_loss = calc_ce_loss(original_logits, batch)\n",
    "\n",
    "                val_original_ce_loss[i*batch_size:(i+1)*batch_size] = ce_loss.cpu() # already max_len - 1\n",
    "                all_activations[i*batch_size:(i+1)*batch_size] = new_x[:, 1:].cpu()\n",
    "                all_tokens[i*batch_size:(i+1)*batch_size] = batch[:, 1:].cpu()\n",
    "            torch.save({\n",
    "                \"ce_loss\": val_original_ce_loss,\n",
    "                \"all_activations\": all_activations,\n",
    "                \"all_tokens\": all_tokens,\n",
    "            }, f\"checkpoints/first_batch_info_skip_first_token.pt\")\n",
    "            # DELETE all tokens\n",
    "            del all_tokens\n",
    "            del all_activations\n",
    "            del val_original_ce_loss\n",
    "            del original_logits\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            # clear cache\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        elif (batch_idx) % save_checkpoints_every == 0:\n",
    "            # if we shoot for 1m tokens then,\n",
    "            # batches_to_run = 1_000_000 // (batch_size*max_length)\n",
    "            num_features = orig_sae.encoder.out_features\n",
    "\n",
    "            original_feature_acts = torch.zeros(batches_to_run*batch_size, max_length-1, num_features, dtype=torch.float16)\n",
    "            new_feature_acts = torch.zeros(batches_to_run*batch_size, max_length-1, num_features, dtype=torch.float16)\n",
    "            val_new_ce_loss = torch.zeros(batches_to_run*batch_size, max_length-1, dtype=torch.float16)\n",
    "            all_activations = torch.zeros(batches_to_run*batch_size, max_length-1, d_model, dtype=torch.float16)\n",
    "\n",
    "            print(\"Saving feature activations\")\n",
    "            for i in tqdm(range(batches_to_run),  total=batches_to_run):\n",
    "                batch = val_data_generator.next().to(device)\n",
    "                with Trace(model, target_layer) as original_trace:\n",
    "                    logits = model(batch, labels=batch).logits\n",
    "                    new_x = original_trace.output[0] if isinstance(original_trace.output, tuple) else original_trace.output\n",
    "                    new_x = new_x.detach()\n",
    "                new_ce_loss = calc_ce_loss(logits, batch)\n",
    "\n",
    "                with Trace(orig_model, target_layer) as original_trace:\n",
    "                    # We could just get the SAE features in first batch, BUT we do need the local_ids of original-SAE for the new one\n",
    "                    _ = orig_model(batch, labels=batch) # Original CE calculated in first batch\n",
    "                    old_x = original_trace.output[0] if isinstance(original_trace.output, tuple) else original_trace.output\n",
    "                    old_x = old_x.detach()\n",
    "\n",
    "                # x = x.normalize - bias (so ready for SAE.encode)\n",
    "                old_x = sub_bias_and_normalize(old_x, orig_sae, batch, normalize=normalize)\n",
    "                new_normalized_x = sub_bias_and_normalize(new_x, sae, batch, normalize=normalize)\n",
    "\n",
    "                original_features, local_mags, local_ids = orig_sae.encode(old_x, return_topk=True)\n",
    "\n",
    "                post_relu_feat_acts_BF = nn.functional.relu(sae.encoder(new_normalized_x - sae.b_dec))\n",
    "                # get the activations at the feature_ids (feature_ids is = to top_indices_BK)\n",
    "                custom_feature_acts = post_relu_feat_acts_BF.gather(dim=-1, index=local_ids)\n",
    "                buffer_BF = torch.zeros_like(post_relu_feat_acts_BF)\n",
    "                custom_features = buffer_BF.scatter_(dim=-1, index=local_ids, src=custom_feature_acts)\n",
    "\n",
    "                # feature_sim = update_feature_sim(feature_sim, per_feature_MSE, local_ids, num_features = post_relu_feat_acts_BF.shape[-1], batch_idx=batch_idx)\n",
    "                start_idx = i*(batch_size)\n",
    "                end_idx = start_idx + (batch_size)  \n",
    "                # Let's ignore the first token for all features in general\n",
    "                val_new_ce_loss[start_idx:end_idx] = new_ce_loss # first token ignored by default\n",
    "                original_feature_acts[start_idx:end_idx] = original_features[:, 1:].cpu() \n",
    "                new_feature_acts[start_idx:end_idx] = custom_features[:, 1:].cpu()\n",
    "                all_activations[start_idx:end_idx] = new_x[:, 1:].cpu()\n",
    "\n",
    "            # save the feature_sim, original_feature_acts, new_feature_acts\n",
    "            # make  dir if not exists\n",
    "            torch.save({\n",
    "                # \"feature_sim\": feature_sim,\n",
    "                \"ce_loss\": val_new_ce_loss,\n",
    "                \"all_activations\": all_activations,\n",
    "                \"original_feature_acts\": original_feature_acts,\n",
    "                \"new_feature_acts\": new_feature_acts}, f\"checkpoints/feature_sim_{batch_idx}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90848c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdca6d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0279a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ce_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (\u001b[43mce_loss\u001b[49m \u001b[38;5;241m-\u001b[39m original_ce_loss)\u001b[38;5;241m.\u001b[39mshape, start_idx \u001b[38;5;241m-\u001b[39m end_idx\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ce_loss' is not defined"
     ]
    }
   ],
   "source": [
    "(ce_loss - original_ce_loss).shape, start_idx - end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8858bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (14) must match the existing size (32) at non-singleton dimension 0.  Target sizes: [14, 127].  Tensor sizes: [32, 127]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mall_ce_diffs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m ce_loss \u001b[38;5;241m-\u001b[39m original_ce_loss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (14) must match the existing size (32) at non-singleton dimension 0.  Target sizes: [14, 127].  Tensor sizes: [32, 127]"
     ]
    }
   ],
   "source": [
    "all_ce_diffs[start_idx:end_idx] = ce_loss - original_ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d765155c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1550, 127]), 1568)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ce_diffs.shape, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604a6ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m feature_mse_avg[feature_mask] \u001b[38;5;241m=\u001b[39m feature_mse_sum[feature_mask] \u001b[38;5;241m/\u001b[39m unique_counts[feature_mask]\n\u001b[1;32m     20\u001b[0m feature_sim[batch_idx, :, unique_indices] \u001b[38;5;241m=\u001b[39m feature_mse_avg[unique_indices]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m feature_mse_avg[unique_indices]\u001b[38;5;241m.\u001b[39mshape, \u001b[43mfeature_sim\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "per_feature_MSE.min()\n",
    "# find out how many times each feature index shows up in local_ids\n",
    "num_features = post_relu_feat_acts_BF.shape[-1]\n",
    "# Get unique indices and their counts\n",
    "unique_indices, counts = torch.unique(local_ids, return_counts=True)\n",
    "\n",
    "unique_counts = torch.zeros(num_features, device=local_ids.device)\n",
    "unique_counts.index_put_((unique_indices,), counts.float())\n",
    "\n",
    "feature_mse_sum = torch.zeros(num_features, device=local_ids.device)\n",
    "feature_mse_sum.scatter_add_(0, local_ids.flatten(), per_feature_MSE.flatten())\n",
    "# set where local_mags=0 to -1 for feature_mse_sum\n",
    "\n",
    "# Compute the average MSE for each feature by dividing by its count\n",
    "# (avoiding division by zero for features that don't appear)\n",
    "feature_mask = unique_counts > 0\n",
    "feature_mse_avg = torch.zeros_like(feature_mse_sum)\n",
    "feature_mse_avg[feature_mask] = feature_mse_sum[feature_mask] / unique_counts[feature_mask]\n",
    "\n",
    "feature_sim[batch_idx, unique_indices] = feature_mse_avg[unique_indices].to(\"cpu\")\n",
    "feature_mse_avg[unique_indices].shape, feature_sim[batch_idx, :, unique_indices].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
