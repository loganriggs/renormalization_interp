{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bart Bussmasn's Mod Addition Code\n",
    "His code from https://www.lesswrong.com/posts/cbDEjnRheYn38Dpc5/interpreting-modular-addition-in-mlps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define the dataset for modular addition\n",
    "p = 113\n",
    "data_set = np.zeros((p*p, 2*p))\n",
    "labels = np.zeros((p*p, p))\n",
    "for number1 in range(p):\n",
    "  for number2 in range(p):\n",
    "    data_set[number1*p + number2][number1] = 1\n",
    "    data_set[number1*p + number2][number2+p] = 1\n",
    "    labels[number1*p + number2][(number1 + number2) % p] = 1\n",
    "\n",
    "# shuffle the dataset\n",
    "shuffle = np.random.permutation(p*p)\n",
    "data_set = data_set[shuffle]\n",
    "labels = labels[shuffle]\n",
    "\n",
    "# divide in train and validation set\n",
    "train_proportion = 0.8\n",
    "train_data = data_set[:int(train_proportion*p*p)]\n",
    "train_labels = labels[:int(train_proportion*p*p)]\n",
    "val_data = data_set[int(train_proportion*p*p):]\n",
    "val_labels = labels[int(train_proportion*p*p):]\n",
    "\n",
    "# convert to tensors\n",
    "train_data = torch.from_numpy(train_data).float().to(device)\n",
    "train_labels = torch.from_numpy(train_labels).float().to(device)\n",
    "val_data = torch.from_numpy(val_data).float().to(device)\n",
    "val_labels = torch.from_numpy(val_labels).float().to(device)\n",
    "\n",
    "# define the 1-hidden layer MLP\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.u1 = torch.zeros(hidden_size).to(device)\n",
    "        self.u2 = torch.zeros(hidden_size).to(device)\n",
    "        self.u3 = torch.zeros(hidden_size).to(device)\n",
    "        self.w1 = torch.zeros(hidden_size).to(device)\n",
    "        self.w2 = torch.zeros(hidden_size).to(device)\n",
    "        self.w3 = torch.zeros(hidden_size).to(device)\n",
    "        self.s1 = torch.zeros(hidden_size).to(device)\n",
    "        self.s2 = torch.zeros(hidden_size).to(device)\n",
    "        self.s3 = torch.zeros(hidden_size).to(device)\n",
    "        self.o1 = torch.zeros(hidden_size).to(device)\n",
    "        self.o2 = torch.zeros(hidden_size).to(device)\n",
    "        self.o3 = torch.zeros(hidden_size).to(device)\n",
    "\n",
    "\n",
    "    def forward(self, x, val=False):\n",
    "        pre_relu = self.fc1(x)\n",
    "        post_relu = torch.nn.functional.relu(pre_relu)\n",
    "        x = self.fc2(post_relu)\n",
    "        return x, pre_relu, post_relu\n",
    "\n",
    "\n",
    "    def cosine(self, x, u, w, s, o):\n",
    "      return u*torch.cos(x*w*2*np.pi + s) + o\n",
    "\n",
    "\n",
    "    def forward_with_replaced_weights(self, x, p_l1=0, plot=True, altered_i=None):\n",
    "        # Cloning the weights before modification\n",
    "        fc1_weight_original = self.fc1.weight.data.clone()\n",
    "        fc2_weight_original = self.fc2.weight.data.clone()\n",
    "\n",
    "        a = torch.argmax(x[:, :113], dim=1).unsqueeze(-1)\n",
    "        b = torch.argmax(x[:, 113:], dim=1).unsqueeze(-1)\n",
    "\n",
    "        pre_relu = self.fc1(x)\n",
    "        if plot:\n",
    "            plt.matshow(pre_relu[:, 0].cpu().detach().numpy().reshape(113, 113))\n",
    "            plt.show()\n",
    "\n",
    "        if not altered_i:\n",
    "          num_altered_neurons = int(p_l1 * pre_relu.shape[-1])\n",
    "          altered_i = torch.randperm(pre_relu.shape[-1])[:num_altered_neurons].to(device)\n",
    "\n",
    "\n",
    "        for i in altered_i:\n",
    "            # Replace incoming weights of the altered neurons\n",
    "            self.fc1.weight.data[i, :113] = self.cosine(torch.arange(113).to(device), self.u1[i], self.w1[i], self.s1[i], self.o1[i])\n",
    "            self.fc1.weight.data[i, 113:] = self.cosine(torch.arange(113).to(device), self.u2[i], self.w2[i], self.s2[i], self.o2[i])\n",
    "\n",
    "            # Replace outgoing weights of the altered neurons\n",
    "            self.fc2.weight.data[:, i] = self.cosine(torch.arange(113).to(device), self.u3[i], self.w3[i], self.s3[i], self.o3[i])\n",
    "\n",
    "        pre_relu = self.fc1(x)  # Re-compute pre_relu with updated weights\n",
    "\n",
    "        if plot:\n",
    "            plt.matshow(pre_relu[:, 0].cpu().detach().numpy().reshape(113, 113))\n",
    "            plt.show()\n",
    "\n",
    "        post_relu = torch.nn.functional.relu(pre_relu)\n",
    "        output = self.fc2(post_relu)\n",
    "\n",
    "        # Restoring the original weights after forward pass\n",
    "        self.fc1.weight.data = fc1_weight_original\n",
    "        self.fc2.weight.data = fc2_weight_original\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def calculate_neuron_properties(self):\n",
    "        for neuron in range(self.hidden_size):\n",
    "          weights1 = self.fc1.weight.detach().cpu().numpy()[neuron, :113]\n",
    "          weights2 = self.fc1.weight.detach().cpu().numpy()[neuron, 113:]\n",
    "          weights3 = self.fc2.weight.detach().cpu().numpy()[:, neuron]\n",
    "          x_data = np.arange(self.output_size)\n",
    "          w1, s1, u1, o1 = self.find_cosine(x_data, weights1)\n",
    "          w2, s2, u2, o2 = self.find_cosine(x_data, weights2)\n",
    "          w3, s3, u3, o3 = self.find_cosine(x_data, weights3)\n",
    "          self.u1[neuron] = u1\n",
    "          self.u2[neuron] = u2\n",
    "          self.u3[neuron] = u3\n",
    "          self.w1[neuron] = w1\n",
    "          self.w2[neuron] = w2\n",
    "          self.w3[neuron] = w3\n",
    "          self.s1[neuron] = s1\n",
    "          self.s2[neuron] = s2\n",
    "          self.s3[neuron] = s3\n",
    "          self.o1[neuron] = torch.tensor(o1)\n",
    "          self.o2[neuron] = torch.tensor(o2)\n",
    "          self.o3[neuron] = torch.tensor(o3)\n",
    "\n",
    "\n",
    "    def find_cosine(self, x_data, y_data):\n",
    "        # Calculate DFT\n",
    "        yf = np.fft.fft(y_data)\n",
    "        xf = np.fft.fftfreq(x_data.size, d=(x_data[1]-x_data[0]))  # assuming x_data is evenly spaced\n",
    "\n",
    "        # Find the peak frequency\n",
    "        idx = np.argmax(np.abs(yf[1:yf.size//2]))  # ignore the zero frequency \"peak\", and only consider the first half of points\n",
    "        freq = np.abs(xf[idx+1])  # shift index by 1 because we ignored the first point\n",
    "\n",
    "        # Calculate phase shift\n",
    "        phase_shift = -np.angle(yf[idx+1])\n",
    "\n",
    "        # Calculate scale\n",
    "        scale = 2 * np.abs(yf[idx+1]) / x_data.size\n",
    "\n",
    "        # Estimate offset\n",
    "        offset = np.mean(y_data)\n",
    "        return freq, phase_shift, scale, offset\n",
    "\n",
    "\n",
    "# define the training loop\n",
    "def train(model, train_data, train_labels, val_data, val_labels, epochs, batch_size, lr):\n",
    "    optimzer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss_values = []\n",
    "    val_loss_values = []\n",
    "    train_acc_values = []\n",
    "    val_acc_values = []\n",
    "\n",
    "    running_train_loss = 0\n",
    "    print(epochs)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct_train_preds = 0\n",
    "        total_train_preds = 0\n",
    "        for batch in range(0, len(train_data), batch_size):\n",
    "            optimzer.zero_grad()\n",
    "            output, pre_relu, post_relu = model(train_data[batch:batch+batch_size])\n",
    "            loss = loss_fn(output, torch.argmax(train_labels[batch:batch+batch_size], axis=1)) #+ 0.0000001*l1_norm\n",
    "            running_train_loss += loss.item()\n",
    "            preds = torch.argmax(output, axis=1)\n",
    "            correct_train_preds += (preds == torch.argmax(train_labels[batch:batch+batch_size], axis=1)).sum().item()\n",
    "            total_train_preds += len(preds)\n",
    "            loss.backward()\n",
    "            optimzer.step()\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        output, _, _ = model(val_data, val=True)\n",
    "        val_loss = loss_fn(output, torch.argmax(val_labels, axis=1)).item()\n",
    "        val_preds = torch.argmax(output, axis=1)\n",
    "        correct_val_preds = (val_preds == torch.argmax(val_labels, axis=1)).sum().item()\n",
    "        total_val_preds = len(val_preds)\n",
    "        avg_train_loss = running_train_loss / (len(train_data) / batch_size)\n",
    "        train_acc = correct_train_preds / total_train_preds\n",
    "        val_acc = correct_val_preds / total_val_preds\n",
    "        train_loss_values.append(avg_train_loss)\n",
    "        val_loss_values.append(val_loss)\n",
    "        train_acc_values.append(train_acc)\n",
    "        val_acc_values.append(val_acc)\n",
    "\n",
    "        print(\"Epoch: {} | Train loss: {} | Validation loss: {} | Train accuracy: {} | Validation accuracy: {}\".format(epoch, avg_train_loss, val_loss, train_acc, val_acc))\n",
    "        running_train_loss = 0\n",
    "    return model, train_loss_values, val_loss_values, train_acc_values, val_acc_values\n",
    "\n",
    "    # def forward_with_replaced_neurons(self, x, p_l1=0, p_l2=0, plot=True):\n",
    "    #     a = torch.argmax(x[:, :113], dim=1).unsqueeze(-1)\n",
    "    #     b = torch.argmax(x[:, 113:], dim=1).unsqueeze(-1)\n",
    "    #        output[:, output_neuron_i] = 0\n",
    "\n",
    "    #         for hidden_neuron_i in range(self.hidden_size):\n",
    "    #             output[:, output_neuron_i] += post_relu[:, hidden_neuron_i] * \\\n",
    "    #                                           self.cosine(output_neuron_i, self.u3[hidden_neuron_i], self.w3[hidden_neuron_i], self.s3[hidden_neuron_i], self.o3[hidden_neuron_i])\n",
    "\n",
    "    #     return output\n",
    "# train the model\n",
    "model = MLP(2*p, 100, p).to(device)\n",
    "num_epochs = 2000\n",
    "model, train_loss_values, val_loss_values, train_acc_values, val_acc_values = train(model, train_data, train_labels, val_data, val_labels, num_epochs, 128, 0.003)\n",
    "model.calculate_neuron_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(5, 5))  # create a new figure with 2 subplots arranged vertically\n",
    "\n",
    "# Plotting the loss values\n",
    "ax[0].semilogy(np.arange(num_epochs), train_loss_values, label='Training Loss')\n",
    "ax[0].semilogy(np.arange(num_epochs), val_loss_values, label='Validation Loss')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Training and Validation Loss')\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].grid(True)\n",
    "ax[0].set_xscale('log')  # Making the x-axis logarithmic\n",
    "\n",
    "# Plotting the accuracy values\n",
    "ax[1].plot(np.arange(num_epochs), train_acc_values, label='Training Accuracy')\n",
    "ax[1].plot(np.arange(num_epochs), val_acc_values, label='Validation Accuracy')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_title('Training and Validation Accuracy')\n",
    "ax[1].legend(loc='lower right')\n",
    "ax[1].grid(True)\n",
    "ax[1].set_xscale('log')  # Making the x-axis logarithmic\n",
    "\n",
    "plt.tight_layout()  # To ensure proper spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the new data\n",
    "new_data = np.zeros((p*p, 2*p))\n",
    "for number1 in range(p):\n",
    "  for number2 in range(p):\n",
    "    new_data[number1*p + number2][number1] = 1\n",
    "    new_data[number1*p + number2][number2+p] = 1\n",
    "\n",
    "# convert to tensor and move to GPU\n",
    "new_data_tensor = torch.from_numpy(new_data).float().to(device)\n",
    "\n",
    "# pass the data through the model and get the post-relu activations\n",
    "model.eval()\n",
    "with torch.no_grad():# generate the new data\n",
    "    _, pre_relu_activations, post_relu_activations = model(new_data_tensor)\n",
    "\n",
    "\n",
    "# Select four random neuron indices\n",
    "random_neurons = np.random.choice(100, 4, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for i, neuron in enumerate(random_neurons):\n",
    "  # get the activations of the neuron\n",
    "  neuron_activations = post_relu_activations[:, neuron].cpu().numpy()\n",
    "\n",
    "  # reshape the activations into a 2D array for plotting\n",
    "  neuron_activations = neuron_activations.reshape(p, p)\n",
    "\n",
    "  # plot the activations as an image\n",
    "  ax = axes[i // 2, i % 2]\n",
    "  im = ax.imshow(neuron_activations, cmap='hot', interpolation='nearest')\n",
    "\n",
    "# Add a colorbar to the figure, for all subplots\n",
    "fig.colorbar(im, ax=axes.ravel().tolist(), orientation='horizontal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save modular model state dict\n",
    "import os\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(model_dir, \"modular_add_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1.weight.data.shape, model.fc2.weight.data.shape, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot TNSE of both model.fc# weights\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Get weights as numpy arrays\n",
    "fc1_weights = model.fc1.weight.data.cpu().numpy()\n",
    "fc2_weights = model.fc2.weight.data.cpu().numpy()\n",
    "\n",
    "# fc1 is shape 113x100. view it as 100 vectors of size 113\n",
    "fc1_weights_reshaped = fc1_weights.reshape(100, 226)\n",
    "# # fc2 is shape 226x100. view it as 100 vectors of size 226\n",
    "fc2_weights_reshaped = fc2_weights.reshape(100, 113)\n",
    "\n",
    "# run t-SNE on both weights\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "fc1_weights_tsne = tsne.fit_transform(fc1_weights_reshaped)\n",
    "fc2_weights_tsne = tsne.fit_transform(fc2_weights_reshaped)\n",
    "\n",
    "# plot the t-SNE of both weights\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(fc1_weights_tsne[:, 0], fc1_weights_tsne[:, 1], c=np.arange(100), cmap='tab10')\n",
    "plt.colorbar(label='Neuron Index')\n",
    "plt.title('t-SNE of FC1 Weights')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(fc2_weights_tsne[:, 0], fc2_weights_tsne[:, 1], c=np.arange(100), cmap='tab10')\n",
    "plt.colorbar(label='Neuron Index')\n",
    "plt.title('t-SNE of FC2 Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do cos-sim between fc1 & self and fc2 & self\n",
    "fc1_weights_norm = np.linalg.norm(fc1_weights_reshaped, axis=1)\n",
    "fc2_weights_norm = np.linalg.norm(fc2_weights_reshaped, axis=1)\n",
    "fc1_weights_normed = fc1_weights_reshaped / fc1_weights_norm[:, np.newaxis]\n",
    "fc2_weights_normed = fc2_weights_reshaped / fc2_weights_norm[:, np.newaxis]\n",
    "\n",
    "# Do cos-sim between fc1 & self and fc2 & self\n",
    "fc1_weights_cos_sim = np.dot(fc1_weights_normed, fc1_weights_normed.T)\n",
    "fc2_weights_cos_sim = np.dot(fc2_weights_normed, fc2_weights_normed.T)\n",
    "\n",
    "# plot the cos-sim of both weights\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(fc1_weights_cos_sim, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Cosine Similarity of Encoder Weights')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(fc2_weights_cos_sim, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Cosine Similarity of Decoder Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a k-means clustering of the fc1 weights\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "fc1_weights_reshaped.shape # (100, 226) 100 vectors of size 226\n",
    "# Fit KMeans with 20 clusters\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "fc1_weights_clusters = kmeans.fit_predict(fc1_weights_reshaped)\n",
    "\n",
    "# Plot the t-SNE with cluster colors\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(fc1_weights_tsne[:, 0], fc1_weights_tsne[:, 1], c=fc1_weights_clusters, cmap='tab20')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title('t-SNE of FC1 Weights Colored by KMeans Clusters')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# repeat for post_relu_activations # shape (d_points, 100) so treat as 100 vectors of size d_points\n",
    "d_points = post_relu_activations.shape[0]\n",
    "post_relu_activations_reshaped = post_relu_activations.reshape(100, d_points).cpu().numpy()\n",
    "\n",
    "\n",
    "# Reshape post_relu_activations for t-SNE\n",
    "post_relu_activations_for_tsne = post_relu_activations_reshaped\n",
    "\n",
    "# Compute t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "post_relu_activations_tsne = tsne.fit_transform(post_relu_activations_for_tsne)\n",
    "\n",
    "# Now plot with the t-SNE coordinates\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(post_relu_activations_tsne[:, 0], post_relu_activations_tsne[:, 1], c=post_relu_activations_clusters, cmap='tab20')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title('t-SNE of Post-ReLU Activations Colored by KMeans Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Also do cos-sim\n",
    "post_relu_activations_norm = np.linalg.norm(post_relu_activations_reshaped, axis=1)\n",
    "post_relu_activations_normed = post_relu_activations_reshaped / post_relu_activations_norm[:, np.newaxis]\n",
    "post_relu_activations_cos_sim = np.dot(post_relu_activations_normed, post_relu_activations_normed.T)\n",
    "\n",
    "# plot the cos-sim\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(post_relu_activations_cos_sim, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.title('Cosine Similarity of Post-ReLU Activations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
