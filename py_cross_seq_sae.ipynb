{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "# Example usage:\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"gpt2\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_bos_token = True\n",
    "batch_size = 128\n",
    "max_length = 128\n",
    "learning_rate = 1e-3\n",
    "target_layer = 'transformer.h.5'\n",
    "\n",
    "dataset_name = \"Elriggs/openwebtext-100k\"\n",
    "# dataset_name = \"prithivMLmods/OpenWeb888K\"\n",
    "def redo_data(num_datapoints=None, batch_size=128):\n",
    "    data_generator = prepare_streaming_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_name=dataset_name,\n",
    "        max_length=max_length,\n",
    "        batch_size=batch_size,\n",
    "        num_datapoints=num_datapoints,  # Optional: limit number of datapoints\n",
    "    )\n",
    "    return data_generator\n",
    "\n",
    "debug = True\n",
    "if(debug):\n",
    "    dataset_name = \"Elriggs/openwebtext-100k\"\n",
    "    num_datapoints = 10_000\n",
    "    total_batches = num_datapoints // batch_size\n",
    "    print(f\"total amount of tokens in dataset: {num_datapoints * 128}\")\n",
    "else:\n",
    "    dataset_name = \"prithivMLmods/OpenWeb888K\"\n",
    "    num_datapoints = None # 880_000\n",
    "    total_batches = 888_000 // batch_size\n",
    "    print(f\"total amount of tokens in dataset: {880_000 * 128}\")\n",
    "\n",
    "data_generator = redo_data(num_datapoints=num_datapoints, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from einops import repeat\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class BiasConfig:\n",
    "    use_positional: bool = False\n",
    "    use_embedding: bool = False\n",
    "    use_shifted_embedding: bool = False\n",
    "    subtract_biases: bool = True\n",
    "\n",
    "def train_with_biases(\n",
    "    model,\n",
    "    sae,\n",
    "    cfg,\n",
    "    data_generator,\n",
    "    embedding_bias,\n",
    "    # positional_bias,\n",
    "    # shifted_embedding_bias,\n",
    "    device,\n",
    "    target_layer,\n",
    "    learning_rate: float = 1e-4,\n",
    "    total_batches: Optional[int] = None,\n",
    "    log_every: int = 10,\n",
    "    print_batch_every: int = 10,\n",
    "):\n",
    "    # Initialize optimizers\n",
    "    bias_params = []\n",
    "    # if bias_config.use_positional:\n",
    "    #     bias_params.extend(positional_bias.parameters())\n",
    "    if cfg.use_embedding:\n",
    "        bias_params.extend(embedding_bias.parameters())\n",
    "    # if bias_config.use_shifted_embedding:\n",
    "    #     bias_params.extend(shifted_embedding_bias.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.Adam(bias_params, lr=learning_rate*10) if bias_params else None\n",
    "    optimizer_sae = torch.optim.Adam(sae.parameters(), lr=learning_rate)\n",
    "    all_optimizers = [opt for opt in [optimizer, optimizer_sae] if opt is not None]\n",
    "    \n",
    "    metrics_history = []\n",
    "    if total_batches is None:\n",
    "        total_batches = 10_000\n",
    "        print(\"tqdm total batches not provided, using made-up value of 10k\")\n",
    "\n",
    "    total_tokens_processed = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(data_generator, total=total_batches)):\n",
    "        for opt in all_optimizers:\n",
    "            opt.zero_grad()\n",
    "        if(cfg.norm_decoder):\n",
    "            sae.set_decoder_norm_to_unit_norm()\n",
    "        \n",
    "        input_ids = batch.to(device)\n",
    "        \n",
    "        # Get original outputs\n",
    "        with torch.no_grad():\n",
    "            with Trace(model, target_layer) as original_trace:\n",
    "                _ = model(input_ids).logits\n",
    "                x = original_trace.output[0] if isinstance(original_trace.output, tuple) else original_trace.output\n",
    "        \n",
    "        # concatenate x by every cfg.tokens_to_combine tokens\n",
    "\n",
    "        # Calculate all biases\n",
    "        all_biases = torch.zeros_like(x)\n",
    "        # if bias_config.use_positional:\n",
    "        #     positions = repeat(torch.arange(max_length), 'l -> b l', b=input_ids.shape[0]).to(device)\n",
    "        #     all_biases = all_biases + positional_bias(positions)\n",
    "        \n",
    "        if cfg.use_embedding:\n",
    "            all_biases = all_biases + embedding_bias(input_ids)\n",
    "            \n",
    "        # if bias_config.use_shifted_embedding:\n",
    "        #     # Shift input_ids right by 1, pad with zeros\n",
    "        #     shifted_ids = torch.nn.functional.pad(input_ids[:, 1:], (1, 0), value=0)\n",
    "        #     shifted_bias = shifted_embedding_bias(shifted_ids)\n",
    "        #     all_biases = all_biases + shifted_bias\n",
    "\n",
    "        if cfg.tokens_to_combine > 1:\n",
    "            x = einops.rearrange(\n",
    "                x,\n",
    "                'b (new_seq tokens_to_combine) d -> b new_seq (tokens_to_combine d)',\n",
    "                tokens_to_combine=cfg.tokens_to_combine,\n",
    "            )\n",
    "            all_biases = einops.rearrange(\n",
    "                all_biases,\n",
    "                'b (new_seq tokens_to_combine) d -> b new_seq (tokens_to_combine d)',\n",
    "                tokens_to_combine=cfg.tokens_to_combine,\n",
    "            )\n",
    "        # Apply SAE with or without bias subtraction\n",
    "        if cfg.subtract_bias:\n",
    "            x_hat = sae(x - all_biases) + all_biases\n",
    "        else:\n",
    "            x_hat = sae(x) + all_biases\n",
    "\n",
    "        # Forward with modified activations\n",
    "        # def modify_activation(act):\n",
    "        #     if(cfg.tokens_to_combine > 1):\n",
    "        #         # Undo the concatenation\n",
    "        #         x_hat = einops.rearrange(\n",
    "        #             x_hat,\n",
    "        #             'b new_seq (tokens_to_combine d) -> b (new_seq tokens_to_combine) d',\n",
    "        #             tokens_to_combine=cfg.tokens_to_combine,\n",
    "        #         )\n",
    "        #     if isinstance(act, tuple):\n",
    "        #         return (x_hat, act[1])\n",
    "        #     return x_hat\n",
    "        \n",
    "        # Calculate losses\n",
    "        mse_loss = (x_hat - x).pow(2).mean()\n",
    "        loss = mse_loss\n",
    "        loss.backward()\n",
    "\n",
    "        if(cfg.norm_decoder):\n",
    "            sae.remove_gradient_parallel_to_decoder_directions()\n",
    "\n",
    "        for opt in all_optimizers:\n",
    "            opt.step()\n",
    "            \n",
    "        # Logging\n",
    "        total_tokens_processed += input_ids.numel()\n",
    "\n",
    "        with torch.no_grad():            \n",
    "            if batch_idx % log_every == 0:\n",
    "                # with Trace(model, target_layer, edit_output=modify_activation) as modified_trace:\n",
    "                #     modified_outputs = model(input_ids).logits\n",
    "                fvu = calculate_fvu(x, x_hat)\n",
    "                # Log to wandb\n",
    "                wandb.log({\n",
    "                    'mse': mse_loss.item(),\n",
    "                    'fvu': fvu.item(),\n",
    "                    'batch': batch_idx,\n",
    "                    \n",
    "                },step =total_tokens_processed)\n",
    "                # ce_loss = F.cross_entropy(modified_outputs[..., :-1, :].reshape(-1, modified_outputs.size(-1)),\n",
    "                #                         input_ids[..., 1:].reshape(-1))\n",
    "                # ce_loss_original = F.cross_entropy(original_outputs[..., :-1, :].reshape(-1, original_outputs.size(-1)),\n",
    "                #                         input_ids[..., 1:].reshape(-1))\n",
    "                # ce_diff = ce_loss_original - ce_loss\n",
    "                metrics_history.append({\n",
    "                    'mse': mse_loss.item(),\n",
    "                    'fvu': fvu.item(),\n",
    "                    # 'ce': ce_loss.item(),\n",
    "                    # 'ce_diff': ce_diff.item(),\n",
    "                })\n",
    "                if batch_idx % print_batch_every == 0:\n",
    "                    print(f\"Batch {batch_idx} - MSE: {mse_loss.item():.4f}, \"\n",
    "                        #   f\"FVU: {fvu.item():.4f}, CE: {ce_loss.item():.4f}, CE Diff: {ce_diff.item():.4f}\")\n",
    "                          f\"FVU: {fvu.item():.4f}\")\n",
    "    \n",
    "    return metrics_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def run_all_configurations(\n",
    "    model,\n",
    "    data_generator,\n",
    "    d_model,\n",
    "    dict_scalar,\n",
    "    k,\n",
    "    device,\n",
    "    target_layer,\n",
    "    learning_rate=1e-3,\n",
    "    total_batches=None,\n",
    "    tokens_to_combine=1,\n",
    "):\n",
    "\n",
    "    # Initialize wandb at the beginning of your main function, before the config loop\n",
    "    wandb_project_name = \"sae-token-combine\"  # Choose a meaningful project name\n",
    "    # wandb.login(\n",
    "    #     key=\"b3d846c89a6848d50969d23df6a4026b4980f95d\"\n",
    "    # )  # Replace with your actual API key\n",
    "\n",
    "    \n",
    "    # # Define all configurations to test\n",
    "    # configs = [\n",
    "    #     BiasConfig(),  # No biases\n",
    "    #     # BiasConfig(use_positional=True),  # Positional only\n",
    "    #     # BiasConfig(use_embedding=True),  # Embedding only\n",
    "    #     # BiasConfig(use_shifted_embedding=True),  # Shifted embedding only\n",
    "    #     # BiasConfig(use_positional=True, use_embedding=True),  # Positional + Embedding\n",
    "    #     # BiasConfig(use_positional=True, use_shifted_embedding=True),  # Positional + Shifted\n",
    "    #     # BiasConfig(use_embedding=True, use_shifted_embedding=True),  # Both embeddings\n",
    "    #     # BiasConfig(use_positional=True, use_embedding=True, use_shifted_embedding=True),  # All biases\n",
    "    # ]\n",
    "    # make a dot dict\n",
    "    all_configs = []\n",
    "    # just vary tokens_to_combine\n",
    "    for use_embedding in [True, False]:\n",
    "    # for norm_decoder in [False]:\n",
    "        for tokens_to_combine in [1, 2, 4, 8, 16, 32, 64]:\n",
    "        # for tokens_to_combine in [128, 256, 512, 1024]:\n",
    "            cfg = DotDict({\n",
    "                'd_model': d_model, \n",
    "                'dict_size': d_model * dict_scalar,\n",
    "                'k': k,\n",
    "                'tokens_to_combine': tokens_to_combine,\n",
    "                \"lr\": learning_rate,\n",
    "                \"use_embedding\": True,\n",
    "                \"subtract_bias\": True,\n",
    "                \"norm_decoder\": False,\n",
    "            })\n",
    "            # cfg.tokens_to_combine = tokens_to_combine\n",
    "            all_configs.append(cfg)\n",
    "\n",
    "\n",
    "    all_results = {}\n",
    "    for cfg in all_configs:\n",
    "        model_save_name = f\"seqComb={cfg.tokens_to_combine}_tokBias={cfg.use_embedding}_normDec={cfg.norm_decoder}\"\n",
    "        wandb.init(\n",
    "            project=wandb_project_name,\n",
    "            name=model_save_name,\n",
    "            config=dict(cfg),  # Log the configuration\n",
    "            reinit=True  # Allow multiple runs in the same process\n",
    "        )\n",
    "        standard_batch_size = 128\n",
    "        if(cfg.tokens_to_combine ==16):\n",
    "            batch_size = standard_batch_size//2\n",
    "        elif(cfg.tokens_to_combine == 32):\n",
    "            print(\"Using 1/4 batch size\")\n",
    "            batch_size = standard_batch_size//4\n",
    "        elif(cfg.tokens_to_combine == 64):\n",
    "            print(\"Using 1/8 batch size\")\n",
    "            batch_size = standard_batch_size//4\n",
    "        elif(cfg.tokens_to_combine == 128):\n",
    "            print(\"Using 1/8 batch size\")\n",
    "            batch_size = standard_batch_size//32\n",
    "        elif(cfg.tokens_to_combine == 256):\n",
    "            print(\"Using 1/8 batch size\")\n",
    "            batch_size = standard_batch_size//32\n",
    "        elif(cfg.tokens_to_combine == 512):\n",
    "            print(\"Using 1/8 batch size\")\n",
    "            batch_size = standard_batch_size//32\n",
    "        elif(cfg.tokens_to_combine == 1024):\n",
    "            print(\"Using 1/8 batch size\")\n",
    "            batch_size = standard_batch_size//64\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            batch_size = standard_batch_size\n",
    "            # batch_size = 128\n",
    "        print(\"batch_size\", batch_size)\n",
    "        \n",
    "        # Log the batch size\n",
    "        wandb.config.update({\"batch_size\": batch_size})\n",
    "\n",
    "        data_generator = redo_data(num_datapoints=num_datapoints, batch_size=batch_size)\n",
    "        print(f\"\\nTraining with config: {cfg}\")\n",
    "        sae = AutoEncoderTopK(activation_dim=d_model*cfg.tokens_to_combine, dict_size=d_model*dict_scalar, k=k).to(device)\n",
    "        embedding_bias = EmbeddingBias(model.transformer.wte).to(device)\n",
    "        # positional_bias = EmbeddingBias(model.transformer.wpe).to(device)\n",
    "        # shifted_embedding_bias = EmbeddingBias(model.transformer.wte).to(device)\n",
    "        \n",
    "        results = train_with_biases(\n",
    "            model=model,\n",
    "            sae=sae,\n",
    "            cfg = cfg,\n",
    "            data_generator=data_generator,\n",
    "            embedding_bias=embedding_bias,\n",
    "            # positional_bias=positional_bias,\n",
    "            # shifted_embedding_bias=shifted_embedding_bias,\n",
    "            device=device,\n",
    "            target_layer=target_layer,\n",
    "            learning_rate=learning_rate,\n",
    "            total_batches=total_batches,\n",
    "            log_every=5,\n",
    "            print_batch_every=50,\n",
    "        )\n",
    "                # Log final metrics\n",
    "        wandb.log({\"final_mse\": results[-1][\"mse\"], \"final_fvu\": results[-1][\"fvu\"]})\n",
    "        wandb.finish()\n",
    "        \n",
    "        all_results[model_save_name] = results\n",
    "        import os\n",
    "        save_dir = \"models\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        # Save the model\n",
    "        model_save_path = f\"{model_save_name}.pt\"\n",
    "        cfg_save_path = f\"{model_save_name}_cfg.json\"\n",
    "        results_save_path = f\"{model_save_name}_results.pt\"\n",
    "        torch.save(sae.state_dict(), os.path.join(save_dir, model_save_path))\n",
    "        with open(os.path.join(save_dir, cfg_save_path), 'w') as f:\n",
    "            json.dump(dict(cfg), f, indent=2)\n",
    "        torch.save(results, os.path.join(save_dir, results_save_path))\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_all_configurations(\n",
    "    model=model,\n",
    "    data_generator=data_generator,\n",
    "    d_model=768,\n",
    "    dict_scalar=8,\n",
    "    k=30,\n",
    "    device=device,\n",
    "    target_layer=target_layer,\n",
    "    max_length=max_length,\n",
    "    total_batches=total_batches,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
